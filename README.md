# News_scraper

This project focuses on answering the 2 questions provided. For that, a scraper was developed and a filtering was applied for the data obtained to obtain relevant information.

# Important information

In order to execute the project, it must be executed in a linux/windows machine because of the logging part. The python version used for it was python 3.11.2.

## Structure of the project

The project has the following structure:

```
ðŸ“¦NEWS_SCRAPER
 â”£ ðŸ“‚config
 â”ƒ â”£ ðŸ“‚logger
 â”ƒ â”ƒ â”£ ðŸ“œhandlers.py
 â”ƒ â”ƒ â”£ ðŸ“œlogger_linux_scraper.yml
 â”ƒ â”ƒ â”— ðŸ“œlogger_windows_scraper.yml
 â”ƒ â”£ ðŸ“œconstants.py
 â”ƒ â”— ðŸ“œmsg.py
 â”£ ðŸ“‚data
 â”ƒ â”£ ðŸ“œanswer.txt
 â”ƒ â”— ðŸ“œdata_extracted.csv
 â”£ ðŸ“‚filter
 â”ƒ â”— ðŸ“œfilter.py
 â”£ ðŸ“‚scraper
 â”ƒ â”— ðŸ“œscraper.py
 â”£ ðŸ“œmain.py
 â”£ ðŸ“œREADME.md
 â”— ðŸ“œrequirements.txt
```

The `config` folder contains all the information related to the logger and constants/errors:
* `logger` subfolder contains the logic for the logger implementation.
* `constants.py` file that contains all the strings used in the project.
* `msg.py` file that contains all the error/info messages for the logger.

The `data` folder contains the file generated by the scraper and the questions answered:
* `answer.txt` Contains the two questions answered.
* `data_extracted.csv` Contains the data extracted from the scraper.

The `scraper` folder contains a file where the scraper's logic has been implemented:
* `scraper.py` That file. It has been object-oriented designed. It mainly generate the csv file.

The `filter` folder contains a file where the filtering's logic has been implemented:
* `filter.py` That file. It mainly generate the answer.txt file extracting the information from the csv file generated by the scraper.


## How to install the necessary packages

To install the dependences of the project, first create a new environment using your preferred way (pyenv, Anaconda, ...).

After that, install the `requirements.txt` file which contains all the basic packages to run the scraper.

```sh
pip install -r requirements.txt
```

## How to execute the project

To start the service you only need to start the `main.py` file in the following way.

```sh
python main.py
```

Thanks for reading <3
